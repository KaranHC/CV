{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "# model_ckpt = \"nateraw/vit-base-beans\"\n",
    "# extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "# model = AutoModel.from_pretrained(model_ckpt)\n",
    "# hidden_dim = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 69.7k/69.7k [00:00<00:00, 324kB/s]\n",
      "c:\\Users\\sri.karan\\.conda\\envs\\azureml\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sri.karan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [00:41<00:00, 8.41MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 160/160 [00:00<00:00, 40.0kB/s]\n",
      "c:\\Users\\sri.karan\\.conda\\envs\\azureml\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:31: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Modify the model's head\n",
    "# model.classifier = nn.Linear(model.config.hidden_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier = nn.Linear(model.config.hidden_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ckpt = \"nateraw/vit-base-beans\"\n",
    "# extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "# model = AutoModel.from_pretrained(model_ckpt)\n",
    "# hidden_dim = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# Data transformation chain.\n",
    "transformation_chain = transforms.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        transforms.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "        transforms.CenterCrop(extractor.size[\"height\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms.functional as functional\n",
    "#http://vision.stanford.edu/aditya86/ImageNetDogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "#         self.transform=transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "        self.transform = transforms.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        transforms.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "        transforms.CenterCrop(extractor.size[\"height\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")\n",
    "        self.image_pairs = self.load_image_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image1_path, image2_path, label = self.image_pairs[idx]\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "        \n",
    "    \n",
    "        # Convert the tensor to a PIL image\n",
    "        # image1 = functional.to_pil_image(image1)\n",
    "        # image2 = functional.to_pil_image(image2)\n",
    "        \n",
    "        image1 = self.transform(image1)\n",
    "        image2 = self.transform(image2)\n",
    "        # image1 = torch.clamp(image1, 0, 1)\n",
    "        # image2 = torch.clamp(image2, 0, 1)\n",
    "        return image1, image2, label\n",
    "\n",
    "    def load_image_pairs(self):\n",
    "        image_pairs = []\n",
    "        # Assume the directory structure is as follows:\n",
    "        # root_dir\n",
    "        # ├── similar\n",
    "        # │   ├── similar_image1.jpg\n",
    "        # │   ├── similar_image2.jpg\n",
    "        # │   └── ...\n",
    "        # └── dissimilar\n",
    "        #     ├── dissimilar_image1.jpg\n",
    "        #     ├── dissimilar_image2.jpg\n",
    "        #     └── ...\n",
    "        similar_dir = os.path.join(self.root_dir, \"similar_all_images\")\n",
    "        dissimilar_dir = os.path.join(self.root_dir, \"dissimilar_all_images\")\n",
    "\n",
    "        # Load similar image pairs with label 1\n",
    "        similar_images = os.listdir(similar_dir)\n",
    "        for i in range(len(similar_images) // 2):\n",
    "            image1_path = os.path.join(similar_dir, f\"similar_{i}_first.jpg\")\n",
    "            image2_path = os.path.join(similar_dir, f\"similar_{i}_second.jpg\")\n",
    "            image_pairs.append((image1_path, image2_path, 1))\n",
    "\n",
    "        # Load dissimilar image pairs with label 0\n",
    "        dissimilar_images = os.listdir(dissimilar_dir)\n",
    "        for i in range(len(dissimilar_images) // 2):\n",
    "            image1_path = os.path.join(dissimilar_dir, f\"dissimilar_{i}_first.jpg\")\n",
    "            image2_path = os.path.join(dissimilar_dir, f\"dissimilar_{i}_second.jpg\")\n",
    "            image_pairs.append((image1_path, image2_path, 0))\n",
    "\n",
    "        return image_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_files(folder_path,num_files):\n",
    "    files=os.listdir(folder_path)\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    picked_files=files[:num_files]\n",
    "    return picked_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(source_folder,files_list,des):\n",
    "    for file in files_list:\n",
    "        source_file=os.path.join(source_folder,file)\n",
    "        \n",
    "        des_file=os.path.join(des,file)\n",
    "        shutil.copy2(source_file,des_file)\n",
    "        \n",
    "        print(f\"Copied {file} to {des}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(source_folder,des):\n",
    "    files_list=os.listdir(source_folder)\n",
    "    for file in files_list:\n",
    "        source_file=os.path.join(source_folder,file)\n",
    "        \n",
    "        des_file=os.path.join(des,file)\n",
    "        shutil.move(source_file,des_file)\n",
    "        \n",
    "        print(f\"Copied {file} to {des}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_file(file_path,new_name):\n",
    "    directory=os.path.dirname(file_path)\n",
    "    new_file_path=os.path.join(directory,new_name)\n",
    "    \n",
    "    os.rename(file_path,new_file_path)\n",
    "    print(f\"File renamed to {new_file_path}\")\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=r\"C:\\Users\\sri.karan\\Downloads\\images1\\Images\\*\"\n",
    "op_path_similar=r\"C:\\Users\\sri.karan\\Downloads\\images1\\Images\\similar_all_images\"\n",
    "tmp=r\"C:\\Users\\sri.karan\\Downloads\\images1\\Images\\tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_path_dissimilar=r\"C:\\Users\\sri.karan\\Downloads\\images1\\Images\\dissimilar_all_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_list=glob.glob(folder_path)\n",
    "folders_list=list(set(folders_list).difference(set(['C:\\\\Users\\\\sri.karan\\\\Downloads\\\\images1\\\\Images\\\\similar_all_images','C:\\\\Users\\\\sri.karan\\\\Downloads\\\\images1\\\\Images\\\\tmp','C:\\\\Users\\\\sri.karan\\\\Downloads\\\\images1\\\\Images\\\\dissimilar_all_images'])))\n",
    "folders_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,g=0,0\n",
    "\n",
    "random.shuffle(folders_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(folder_path):\n",
    "    if i in ['C:\\\\Users\\\\sri.karan\\\\Downloads\\\\images1\\\\Images\\\\similar_all_images','C:\\\\Users\\\\sri.karan\\\\Downloads\\\\images1\\\\Images\\\\tmp','C:\\\\Users\\\\sri.karan\\\\Downloads\\\\images1\\\\Images\\\\dissimilar_all_images']:\n",
    "        continue\n",
    "    \n",
    "    file_name=i.split('\\\\')[-1].split(\"-\")[1]\n",
    "    picked_files=pick_random_files(i,6)\n",
    "    \n",
    "    copy_files(i,picked_files,tmp)\n",
    "    \n",
    "    for m in range(3):\n",
    "        rename_file(os.path.join(tmp,picked_files[m*2]),\"similar_\"+str(g)+\"_first.jpg\")\n",
    "        rename_file(os.path.join(tmp,picked_files[m*2+1]),\"similar_\"+str(g)+\"_second.jpg\")\n",
    "        g+=1\n",
    "    move_files(tmp,op_path_similar)\n",
    "    choice_one,choice_two=random.choice(range(len(folders_list))),random.choice(range(len(folders_list)))\n",
    "    picked_dissimilar_one=pick_random_files(folders_list[choice_one],3)\n",
    "    picked_dissimilar_two=pick_random_files(folders_list[choice_two],3)\n",
    "    copy_files(folders_list[choice_one],picked_dissimilar_one,tmp)\n",
    "    copy_files(folders_list[choice_two],picked_dissimilar_two,tmp)\n",
    "    picked_files_dissimilar=picked_dissimilar_one+picked_dissimilar_two\n",
    "\n",
    "    for m in range(3):\n",
    "        rename_file(os.path.join(tmp,picked_files_dissimilar[m]),\"dissimilar_\"+str(l)+\"_first.jpg\")\n",
    "        rename_file(os.path.join(tmp,picked_files_dissimilar[m+3]),\"dissimilar_\"+str(l)+\"_second.jpg\")\n",
    "        l+=1\n",
    "    move_files(tmp,op_path_dissimilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImagePairDataset(r\"C:\\Users\\sri.karan\\Downloads\\images1\\Images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "        image1, image2, labels = batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]),\n",
       " torch.Size([32, 3, 224, 224]),\n",
       " torch.Size([32]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.shape,image2.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=model.vit(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=op.last_hidden_state[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.view(batch_size, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(output.view(batch_size, -1), output.view(batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier = nn.Linear(model.config.hidden_size, 2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 0.5517\n",
      "Epoch [2/10], Average Loss: 0.4882\n",
      "Epoch [3/10], Average Loss: 0.4533\n",
      "Epoch [4/10], Average Loss: 0.4243\n",
      "Epoch [5/10], Average Loss: 0.3954\n",
      "Epoch [6/10], Average Loss: 0.3753\n",
      "Epoch [7/10], Average Loss: 0.3643\n",
      "Epoch [8/10], Average Loss: 0.3476\n",
      "Epoch [9/10], Average Loss: 0.3380\n",
      "Epoch [10/10], Average Loss: 0.3302\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        image1, image2, labels = batch\n",
    "        # image1 = torch.clamp(image1, 0, 1)\n",
    "        # image2 = torch.clamp(image2, 0, 1)\n",
    "    \n",
    "        # # # Convert the tensor to a PIL image\n",
    "        # image1 = functional.to_pil_image(image1)\n",
    "        # image2 = functional.to_pil_image(image2)\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract features from the images\n",
    "        # inputs = extractor(images=image1, return_tensors=\"pt\").to(device)\n",
    "        # targets = extractor(images=image2, return_tensors=\"pt\").to(device)\n",
    "        # inputs = extractor(image1)['pixel_values']\n",
    "        # targets = extractor(image2)['pixel_values']\n",
    "        # inputs = image1\n",
    "        # targets = image2\n",
    "\n",
    "        # Forward pass\n",
    "        # outputs = model(pixel_values=inputs, labels=targets)\n",
    "        outputs1 = model.vit(image1).last_hidden_state[:,0]\n",
    "        outputs2 = model.vit(image2).last_hidden_state[:,0]\n",
    "        outputs1 = outputs1.view(batch_size, -1)\n",
    "        outputs2 = outputs2.view(batch_size, -1)\n",
    "        similarity_scores = torch.cosine_similarity(outputs1, outputs2)\n",
    "        # loss = criterion(outputs.logits, labels.float().unsqueeze(1))\n",
    "        # loss=criterion(outputs1, outputs2, labels)\n",
    "        loss = criterion(similarity_scores, labels.float())\n",
    "        # Backward pass and optimization\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for name,par in model.named_parameters():\n",
    "            if par.requires_grad:\n",
    "                # print(name,par.data.sum())\n",
    "                pass\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,par in model.named_parameters():\n",
    "    if par.requires_grad:\n",
    "        print(name,par.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        image1, image2, labels = batch\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Extract features from the images\n",
    "        # inputs = extractor(images=image1, return_tensors=\"pt\").to(device)\n",
    "        # targets = extractor(images=image2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # inputs = image1\n",
    "        # targets = image2\n",
    "        # Forward pass\n",
    "        # outputs = model(pixel_values=inputs, labels=targets)\n",
    "        outputs1 = model.vit(image1).last_hidden_state[:,0]\n",
    "        outputs2 = model.vit(image2).last_hidden_state[:,0]\n",
    "        # predicted_labels = torch.round(torch.sigmoid(outputs.logits))\n",
    "        similarity_scores = torch.cosine_similarity(outputs1, outputs2)\n",
    "        predictions = (similarity_scores > 0.5).long()\n",
    "        # total_correct += (predictions == labels.unsqueeze(1)).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += torch.sum(predictions == labels).item()\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"image_similarity_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# Initialize the ViT model and feature extractor\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Load the saved model state dict\n",
    "model.load_state_dict(torch.load(\"image_similarity_model.pt\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
